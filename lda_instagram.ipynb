{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Implementing LDA in Python</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Dr. W.J.B. Mattingly</center>\n",
    "\n",
    "<center>Smithsonian Data Science Lab and United States Holocaust Memorial Museum</center>\n",
    "\n",
    "<center>February 2021</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts in this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO list :\n",
    "\n",
    "- Check this : Latent-Dirichlet-Allocation-LDA\n",
    "- INSTA automatic comment extraction\n",
    "- install on ania laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#1introduction\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Export panda\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f) \n",
    "    return (data)\n",
    "\n",
    "def write_data(file, data):\n",
    "    with open (file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print (stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from INSTAGRAM\n",
    "\n",
    "filename = \"all_data.xlsx\"\n",
    "xl = pd.ExcelFile(filename)\n",
    "table = xl.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# pre-treat the data\n",
    "#####################\n",
    "\n",
    "# emoji_list = []\n",
    "# counter = 0\n",
    "# for elt in table.get(\"COMMENT\")[:]:\n",
    "#     for c in elt :\n",
    "#         if len(c.encode('utf8')) > 1:\n",
    "#             if not(c in emoji_list) :\n",
    "#                 emoji_list.append(c)\n",
    "#                 counter += 1\n",
    "\n",
    "#                 counter = 0\n",
    "\n",
    "\n",
    "# Extract comments as a list of separate words                \n",
    "# word_list, world_list_clean, emoji_list = [], [], [] \n",
    "\n",
    "# for elt in table.get(\"COMMENT\")[:]:\n",
    "#     word_list.append(elt.split())\n",
    "    \n",
    "# for line in word_list:\n",
    "#     new_line = []\n",
    "#     for word in line :\n",
    "#         new_word = \"\"\n",
    "#         for id,character in enumerate(word):\n",
    "#             if character.encode(\"utf-8\")[0] > 122 :\n",
    "#                 if not (character in emoji_list):\n",
    "#                     emoji_list.append(character)\n",
    "#                 # TODO : add a new word corresponding to the emoji\n",
    "#                 # new_word += \"FIRERERE\"\n",
    "#             else:\n",
    "#                 new_word += character\n",
    "#         if len(new_word) > 0:\n",
    "#             new_line.append(new_word)\n",
    "#     if len(new_line) > 0 :\n",
    "#          world_list_clean.append(new_line)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus,emoji_list = [], []\n",
    "for line in table.get(\"COMMENT\")[:]:\n",
    "    new_line = \"\"\n",
    "    for id,character in enumerate(line):\n",
    "        if character.encode(\"utf-8\")[0] > 122 :\n",
    "            if not (character in emoji_list):\n",
    "                emoji_list.append(character)\n",
    "            # TODO : add a new word corresponding to the emoji\n",
    "            # new_word += \"FIRERERE\"\n",
    "        else:\n",
    "            new_line += character\n",
    "    if len(new_line) > 0:\n",
    "        corpus.append(new_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process using \n",
    "\n",
    "https://github.com/sethns/Latent-Dirichlet-Allocation-LDA-/blob/main/Topic%20Modeling%20_%20Extracting%20Topics_%20Using%20Genism.ipynb?fbclid=IwAR2renok7J_JmL8yWYHWV3cu69zdNiWpmwv-qz2PxWLaBNIHahFQuRyAg_U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Text Preprocessing\n",
    "\n",
    "Steps to preprocess text data:\n",
    "\n",
    "   -  Convert the text into lowercase\n",
    "   - Split text into words\n",
    "   -  Remove the stop loss words\n",
    "   -  Remove the Punctuation, any symbols and special characters\n",
    "   - Normalize the word (I'll be using Lemmatization for normalization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# PRE-PROCESS 2.0\n",
    "# using LDA\n",
    "##################\n",
    "# stop loss words \n",
    "# stop = set(stopwords.words('english'))\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# punctuation \n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# lemmatization\n",
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "# One function for all the steps:\n",
    "def clean(doc):\n",
    "    \n",
    "    # convert text into lower case + split into words\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stopwords])\n",
    "    \n",
    "    # remove any stop words present\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)  \n",
    "    \n",
    "    # remove punctuations + normalize the text\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
    "    return normalized\n",
    "\n",
    "# clean data stored in a new list\n",
    "clean_corpus = [clean(doc).split() for doc in corpus] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beautiful']\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# PRE-PROCESS 2.1\n",
    "##################\n",
    "\n",
    "# def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "#     nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "#     texts_out = []\n",
    "#     for text in texts:\n",
    "#         doc = nlp(text)\n",
    "#         new_text = []\n",
    "#         for token in doc:\n",
    "#             if token.pos_ in allowed_postags:\n",
    "#                 new_text.append(token.lemma_)\n",
    "#         final = \" \".join(new_text)\n",
    "#         texts_out.append(final)\n",
    "#     return (texts_out)\n",
    "\n",
    "# def gen_words(texts):\n",
    "#     final = []\n",
    "#     for text in texts:\n",
    "#         new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "#         final.append(new)\n",
    "#     return (final)\n",
    "\n",
    "# lemmatized_texts = lemmatization(corpus)\n",
    "# clean_corpus = gen_words(lemmatized_texts)\n",
    "\n",
    "# print (clean_corpus[0][0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data to matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1)]\n",
      "beautiful\n"
     ]
    }
   ],
   "source": [
    "# Version 1.\n",
    "\n",
    "# id2word = corpora.Dictionary(clean_corpus)\n",
    "\n",
    "# corpus2 = []\n",
    "# for text in clean_corpus:\n",
    "#     new = id2word.doc2bow(text)\n",
    "#     corpus2.append(new)\n",
    "\n",
    "# print (corpus2[0][0:20])\n",
    "\n",
    "# word = id2word[[0][:1][0]]\n",
    "# print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beautiful']\n"
     ]
    }
   ],
   "source": [
    "# Version 2, more efficient\n",
    "\n",
    "#BIGRAMS AND TRIGRAMS\n",
    "data_words = clean_corpus\n",
    "bigram_phrases = gensim.models.Phrases(clean_corpus, min_count=5, threshold=100)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=100)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return([bigram[doc] for doc in texts])\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return ([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "data_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "print (data_bigrams_trigrams[0][0:20])\n",
    "\n",
    "#TF-IDF REMOVAL\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "corpus2 = [id2word.doc2bow(text) for text in texts]\n",
    "# print (corpus[0][0:20])\n",
    "\n",
    "tfidf = TfidfModel(corpus2, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words  = []\n",
    "words_missing_in_tfidf = []\n",
    "for i in range(0, len(corpus2)):\n",
    "    bow = corpus2[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus2[i] = new_bow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus2,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=8,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas_cbrs/.local/lib/python3.8/site-packages/pyLDAvis/_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el57941397610459420482788394313\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el57941397610459420482788394313_data = {\"mdsDat\": {\"x\": [-0.3797942033163787, 0.20292119701894398, 0.3575452746644287, 0.12620118528687213, -0.19143566612295826, -0.07405816164684362, 0.08043465860388976, -0.12181428448795406], \"y\": [0.061971335913884124, 0.27343266036266906, -0.05738389283968388, -0.02607036514012131, -0.24608299710703452, 0.29259773241420656, -0.31609640422048957, 0.017631930616569525], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [22.471195732171864, 16.946674733512772, 13.130845347775457, 10.685817133444257, 10.169309260875938, 9.342909577037457, 9.32414776409905, 7.929100451083207]}, \"tinfo\": {\"Term\": [\"hm\", \"please\", \"love\", \"animal\", \"u\", \"stop\", \"order\", \"one\", \"customer\", \"need\", \"boot\", \"dm\", \"beautiful\", \"fashion_friend\", \"hi\", \"good\", \"service\", \"make\", \"purchase\", \"using\", \"you\", \"were\", \"received\", \"able\", \"sweatshirt\", \"look\", \"new\", \"brand\", \"help\", \"hope\", \"please\", \"need\", \"dm\", \"boot\", \"shopping\", \"soon\", \"there\", \"hear\", \"like\", \"number\", \"happy\", \"model\", \"color\", \"email\", \"sorry\", \"see\", \"address\", \"price\", \"check\", \"hi\", \"send\", \"find\", \"chat\", \"case\", \"website\", \"made\", \"let\", \"article\", \"style\", \"out\", \"look\", \"u\", \"product\", \"were\", \"received\", \"return\", \"refund\", \"say\", \"want\", \"go\", \"experience\", \"week\", \"got\", \"never\", \"collection\", \"even\", \"work\", \"ordered\", \"reply\", \"order\", \"option\", \"sell\", \"delivery\", \"either\", \"second\", \"ive\", \"pay\", \"place\", \"compliment\", \"discount\", \"returned\", \"peta\", \"reach\", \"tell\", \"vegan\", \"item\", \"time\", \"free\", \"get\", \"customer\", \"service\", \"still\", \"first\", \"feel\", \"take\", \"message\", \"deliver\", \"dont\", \"parcel\", \"valuable\", \"month\", \"im\", \"write\", \"lost\", \"fit\", \"bad\", \"card\", \"helpful\", \"twice\", \"came\", \"original\", \"credit\", \"bank\", \"jacket\", \"late\", \"disappointed\", \"app\", \"telling\", \"oh\", \"one\", \"store\", \"2\", \"get\", \"hm\", \"exchange\", \"best\", \"product\", \"it\", \"animal\", \"stop\", \"using\", \"new\", \"boycott\", \"abuse\", \"clothing\", \"skin\", \"ever\", \"omg\", \"reza\", \"trendsetter\", \"ahhhh\", \"kneehigh\", \"selling\", \"exploiting\", \"outfit\", \"cashmere\", \"murdered\", \"yes\", \"agree\", \"cruel\", \"feather\", \"cruelty\", \"high\", \"more\", \"cool\", \"stopanimalabuse\", \"industry\", \"hair\", \"love\", \"thanks\", \"store\", \"hm\", \"orange\", \"company\", \"fashion_friend\", \"able\", \"glad\", \"wow\", \"back\", \"much\", \"thank\", \"pink\", \"day\", \"fashion\", \"feedback\", \"wanted\", \"assist\", \"eye\", \"country\", \"similar\", \"yours\", \"caught\", \"far\", \"inspo\", \"positive\", \"wait\", \"shop\", \"friend\", \"know\", \"kindly\", \"right\", \"wish\", \"issue\", \"talk\", \"great\", \"hope\", \"were\", \"love\", \"hey\", \"best\", \"employee\", \"quality\", \"proof\", \"way\", \"receipt\", \"would\", \"information\", \"put\", \"post\", \"statement\", \"turn\", \"asset\", \"digital\", \"always\", \"remove\", \"forum\", \"gdansk\", \"girlfriend\", \"laughed\", \"morning\", \"refused\", \"vinted\", \"sticker\", \"think\", \"colour\", \"overshirt\", \"shame\", \"washed\", \"worn\", \"purchase\", \"hm\", \"sent\", \"it\", \"exchange\", \"india\", \"payment\", \"one\", \"beautiful\", \"make\", \"you\", \"sweatshirt\", \"sure\", \"online\", \"use\", \"hello\", \"fashionista\", \"also\", \"fan\", \"share\", \"said\", \"direct\", \"confirmation\", \"pain\", \"doubt\", \"this\", \"part\", \"walkin\", \"understand\", \"human\", \"non\", \"tag\", \"beaultiful\", \"appreciate\", \"bug\", \"google\", \"worst\", \"complete\", \"u\", \"thanks\", \"good\", \"brand\", \"help\", \"amazing\", \"buy\", \"nice\", \"smaller\", \"give\", \"shoe\", \"could\", \"later\", \"show\", \"body\", \"term\", \"regard\", \"size\", \"bigger\", \"diversity\", \"perfect\", \"bonus\", \"site\", \"bag\", \"h\", \"long\", \"wardrobe\", \"winter\", \"heat\", \"life\", \"comment\", \"addition\", \"available\", \"hm\"], \"Freq\": [112.0, 103.0, 71.0, 44.0, 81.0, 39.0, 51.0, 46.0, 32.0, 42.0, 42.0, 41.0, 24.0, 24.0, 43.0, 20.0, 25.0, 21.0, 25.0, 22.0, 21.0, 34.0, 28.0, 20.0, 19.0, 44.0, 20.0, 16.0, 16.0, 27.0, 103.20945734064001, 41.58912091706507, 41.074622872877036, 42.06648588848808, 29.47052530997789, 29.281828283673942, 28.43085678376974, 27.669260446465017, 26.984129373904896, 24.263334168124853, 23.770931383990526, 20.738120050197537, 19.378372871732743, 18.39777931511042, 17.70464322951894, 17.246940246141982, 17.233138924495893, 15.54286385421882, 15.099767349460654, 41.40010073252037, 14.676328724804803, 13.991687866864117, 13.792508737454623, 13.302882923287381, 12.290098150217206, 12.115748802076538, 11.717181038126897, 11.334363130469875, 10.556489066102163, 10.472498048429006, 39.66408097008824, 69.11578094322812, 27.276779285820947, 16.664999350973222, 27.93758252336321, 22.539942631066197, 22.241443183268142, 21.006857331989792, 18.99233510179921, 18.366741575025998, 17.950340132617903, 16.301160011360494, 15.565705890951458, 14.624141173336298, 14.466060055944075, 14.2305080381941, 14.098498739268049, 11.696050940801136, 11.589455649577086, 48.79047909109898, 11.407557032240167, 11.243191085474747, 10.968309832319214, 10.722847280968422, 10.453635385890257, 10.413681385159233, 10.238195648270482, 10.20222155756317, 9.311616004436864, 9.199080795606074, 9.066167910465506, 9.085696732068618, 9.110631580909436, 8.957571304860046, 17.321145685696095, 12.882358438805353, 10.239852050885437, 11.805563524660624, 14.655191493676964, 31.48034976665741, 25.322287873247255, 15.46321202965446, 12.734546214154859, 12.84907764348757, 12.259454181606744, 12.096132193747154, 11.515237534578329, 11.132048776564554, 11.09211344396248, 10.718677676713815, 10.277927149674582, 9.177159435629298, 8.663348293792176, 8.44242292926747, 8.345539540324891, 7.74628711045101, 7.411384018259809, 6.916972438413014, 6.9168116088827984, 6.498181543423422, 6.300680479878996, 6.155526627905254, 6.149560332422438, 6.0366035922213825, 5.982117587942918, 5.98172711624767, 6.011320149880637, 5.8475804853306395, 5.72823497207486, 36.69617465414593, 15.73198778007917, 9.927852471799792, 13.798008678512577, 19.965173043244736, 8.538670201411442, 7.822746749350155, 8.126702559252228, 6.372170406279814, 43.52218358358292, 38.62421451687338, 22.075159770863365, 19.79995955399001, 13.087843605621154, 11.365690686417206, 11.363859636838189, 8.910119714053982, 8.91474357004993, 8.20102167182171, 7.087662133327457, 6.534697532420689, 6.39045028562413, 6.385819918072069, 6.073734056681007, 5.657269904262983, 5.244362689371767, 5.188159495092334, 5.182126863931306, 4.480356926948774, 4.180776834650823, 4.128675920722674, 3.8940424146750896, 3.5275433857801004, 3.4894406896061967, 3.33853700457297, 3.2541723068375794, 3.242187136655715, 3.1509706540002624, 3.0700512206043395, 53.6908588166534, 14.204333249139, 7.908032251043685, 8.49431453111461, 3.565290225412421, 3.750002235362929, 23.802894493787885, 20.073324261125578, 17.922017027014782, 15.307976293833208, 13.389177262511582, 12.244225287195096, 11.701355820766945, 11.484110372397208, 11.214143664567608, 10.540932878059655, 8.919377073499017, 8.354124092966755, 8.31034235828378, 8.237454185310886, 8.10899359846953, 7.769447196994903, 6.817754659682221, 6.563353110825714, 6.561186084751627, 6.561186084751627, 6.561186084751627, 6.490474852756219, 6.204687057509546, 5.34090501118177, 5.289991982229253, 4.712933759997138, 4.332445004300348, 4.285750456903263, 4.140219504816594, 4.026855597762741, 12.064516423730053, 16.10636769320192, 17.25103089695692, 17.147653156669985, 6.082478436942558, 5.979575992337297, 15.035571262198088, 14.85900465341845, 14.007774763167903, 11.753482131625937, 11.680960551407168, 9.551775644739314, 8.036715562688318, 7.6336799919760425, 6.698488532866919, 6.471753696807934, 6.2650317243234745, 5.88299622432723, 5.88299622432723, 5.88417016961315, 5.881966317914897, 5.881676532196889, 5.881676532196889, 5.881676532196889, 5.881676532196889, 5.881676532196889, 5.881676532196889, 5.881676532196889, 4.470157911972816, 3.9586878276587183, 3.159422950003892, 2.810545141775175, 2.810545141775175, 2.810545141775175, 2.810545141775175, 20.897765540425922, 78.58278156522874, 6.2813513856333225, 7.735319628280357, 8.90292777308643, 4.557944766173827, 5.1590289488185, 5.310466182731388, 23.84049677385402, 21.07099719491136, 20.427942057621628, 18.959520895951144, 13.55554411240141, 13.10337029434577, 12.819749444797747, 12.2674963428864, 10.524631580406604, 8.901471618664884, 8.471101720888356, 7.807448910166486, 7.597528436153691, 7.393667037212074, 6.834586680419324, 6.657250780324101, 6.638691169746454, 6.31276190806179, 5.994002513999331, 5.994002513999331, 5.908272624453694, 5.560319556701045, 5.560319556701045, 5.2108806973775845, 5.154669480672617, 4.54057307450538, 4.46332895823319, 4.46332895823319, 3.847514942919275, 3.6457524021277825, 12.09812405833859, 6.92533246594474, 20.30999760403974, 16.190978286077833, 16.145798887165537, 15.07848160991192, 14.578830487519824, 13.200390627877836, 11.705716224155948, 10.412147304737475, 10.202612417043646, 8.978941931127261, 7.80181483619993, 7.557845308999743, 6.57013192357122, 6.527145638380643, 6.40617891633412, 6.358763884641393, 5.889945361042852, 5.889945361042852, 5.761318927202898, 5.204132753972585, 4.855795365443187, 7.6114971135322875, 3.866171521399184, 3.3897027078440862, 3.3122639058156462, 2.867865333882521, 2.54593493825682, 2.5269861747887763, 2.41453727644795, 2.372118958911014, 6.705092247646467, 4.733408120298659], \"Total\": [112.0, 103.0, 71.0, 44.0, 81.0, 39.0, 51.0, 46.0, 32.0, 42.0, 42.0, 41.0, 24.0, 24.0, 43.0, 20.0, 25.0, 21.0, 25.0, 22.0, 21.0, 34.0, 28.0, 20.0, 19.0, 44.0, 20.0, 16.0, 16.0, 27.0, 103.7860044971272, 42.165083924021815, 41.650051596947115, 42.85338085520895, 30.04649446974675, 29.857295885464953, 29.00645851536312, 28.2449481368244, 27.571264972894916, 24.83914782747766, 24.346881045785526, 21.31671765867321, 19.954157715982472, 18.973737100004456, 18.280103753857443, 17.82267706219174, 17.808593797322715, 16.118956175582916, 15.675231102006032, 43.013304092777005, 15.252177731581437, 14.569133701905464, 14.36790453627061, 13.882216905948248, 12.865927037705468, 12.693268112487438, 12.292714662125835, 11.909964234254886, 11.132243495941532, 11.04912686212996, 44.37738317678187, 81.97242468346366, 35.909555506239066, 34.406234056318574, 28.515421678993977, 23.11880557136685, 22.818992360841825, 21.58604810952998, 19.571222581666536, 18.94586774640068, 18.533123759814565, 16.88042830341528, 16.143687481297874, 15.201715685059302, 15.044895501897381, 14.810669052373676, 14.678023953961375, 12.274569414416007, 12.167476653677491, 51.23752752971835, 11.988719533859795, 11.821862148243007, 11.546458740626624, 11.30806327558694, 11.032555239498834, 10.99163535399228, 10.815700219338467, 10.78483362966441, 9.893029706507434, 9.776627771399472, 9.643793083309744, 9.665442381571195, 9.696027003107275, 9.53714234669077, 20.00251691094637, 14.914077608493107, 11.560539038959266, 15.570196821013063, 28.961157191080833, 32.094282912604, 25.935996862351242, 16.077166725011658, 13.34846239834321, 13.46856578839304, 12.873769946254658, 12.712568767292993, 12.129480654959645, 11.748127441854662, 11.706188552394103, 11.333896232401193, 10.891842973874503, 9.790880339258386, 9.27761722344544, 9.056145908520698, 8.960602802628188, 8.360034005476015, 8.025126760079402, 7.5306878085166105, 7.530657284107837, 7.117367020527847, 6.914684367844326, 6.76964512754503, 6.768062323474481, 6.650706971311853, 6.59586887371291, 6.595801232239289, 6.638643933899403, 6.461649327470253, 6.3428909275089955, 46.23675897913859, 24.17179061779962, 15.398201920011857, 28.961157191080833, 112.15371995735174, 17.980079431255547, 14.415910419522259, 35.909555506239066, 14.645623866602415, 44.122901727373886, 39.22486780097709, 22.677171276254228, 20.40494638217961, 13.689813996851571, 11.966591961199867, 11.965193703570609, 9.512201585538335, 9.524138370152526, 8.801564663953101, 7.688240921648903, 7.137196567134175, 6.991075474174798, 6.989203535416334, 6.674269459624458, 6.257859331674171, 5.8460318108298965, 5.788683030658403, 5.78441058556021, 5.081577536408707, 4.781625691719671, 4.729549956183459, 4.494571345008651, 4.128068316885489, 4.091969665060678, 3.9394275357966797, 3.8551890007146206, 3.842715023759518, 3.7514860674852866, 3.6707135883701296, 71.3544639868698, 32.571567838768374, 24.17179061779962, 112.15371995735174, 5.630647946091564, 12.948957843579649, 24.40138410048997, 20.676227520613146, 18.520518086770597, 15.906318288530029, 13.98958039664995, 12.843109540858372, 12.300043186713275, 12.082732904840634, 11.812907046691068, 11.139734719203616, 9.518131884827756, 8.953847433107587, 8.90958149544526, 8.83598520872083, 8.709025269136461, 8.369808996855097, 7.416249196457075, 7.162369326467043, 7.161712550522184, 7.161712550522184, 7.161712550522184, 7.090696367548779, 6.804559026843458, 5.939436019460265, 5.888549699163013, 5.311919540875257, 4.9320478713661196, 4.884602450152734, 4.739525410384274, 4.627629075842487, 16.627177527099025, 27.649805722651777, 34.406234056318574, 71.3544639868698, 14.1067432580351, 14.415910419522259, 15.644356843239716, 15.470429279845371, 14.615622845042678, 12.363873843157016, 12.296289234928302, 10.160050298123434, 8.646401485462688, 8.242163365445604, 7.306482446407125, 7.079461229736006, 6.872460418997696, 6.490577995478684, 6.490577995478684, 6.492400730893681, 6.490011697379974, 6.490275511789416, 6.490275511789416, 6.490275511789416, 6.490275511789416, 6.490275511789416, 6.490275511789416, 6.490275511789416, 5.077608388031391, 4.566202804295211, 3.766614048848895, 3.4180043746863564, 3.4180043746863564, 3.4180043746863564, 3.4180043746863564, 25.490289970858527, 112.15371995735174, 9.656677677425936, 14.645623866602415, 17.980079431255547, 7.309625415975428, 11.794268006552667, 46.23675897913859, 24.446249881005013, 21.677961599526427, 21.035765690617506, 19.57194992624093, 14.163478474750654, 13.709779022451071, 13.42605678374728, 12.873519947325763, 11.13079672659557, 9.51066764743703, 9.077180593174967, 8.416297168748414, 8.205873399156268, 8.001769991967857, 7.448215810810128, 7.263559348807294, 7.253898164835936, 6.919210983194691, 6.603834035321689, 6.603834035321689, 6.51523762210021, 6.166673327421971, 6.166673327421971, 5.818755055069303, 5.76024279105016, 5.146463289716347, 5.069755053022484, 5.069755053022484, 4.454914995507777, 4.253514778460608, 81.97242468346366, 32.571567838768374, 20.917988349388892, 16.800560050603206, 16.7559043328306, 15.686015129103568, 15.187095520158083, 13.807617880996334, 12.318171837339904, 11.020028129298055, 10.811233261028798, 9.587583470318602, 8.412975497083176, 8.166243660557656, 7.1780733562055685, 7.140104551449811, 7.020017503388847, 6.969897599977481, 6.500132813591657, 6.500132813591657, 6.368453001912998, 5.813023470841385, 5.463441144755826, 8.778515207094303, 4.510502565108026, 4.000858199967864, 3.9192867092191577, 3.4759657287101113, 3.15296582981658, 3.134115904671405, 3.021986130310399, 2.992337416876481, 10.670965992220044, 112.15371995735174], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.6565, -3.5654, -3.5778, -3.554, -3.9098, -3.9162, -3.9457, -3.9729, -3.998, -4.1042, -4.1248, -4.2612, -4.3291, -4.381, -4.4194, -4.4456, -4.4464, -4.5496, -4.5785, -3.5699, -4.607, -4.6548, -4.6691, -4.7052, -4.7844, -4.7987, -4.8322, -4.8654, -4.9365, -4.9445, -3.6128, -3.0574, -3.9872, -4.4799, -3.6811, -3.8958, -3.9091, -3.9662, -4.067, -4.1005, -4.1234, -4.2198, -4.266, -4.3284, -4.3392, -4.3557, -4.365, -4.5518, -4.561, -3.1235, -4.5768, -4.5913, -4.616, -4.6387, -4.6641, -4.6679, -4.6849, -4.6884, -4.7798, -4.7919, -4.8065, -4.8044, -4.8016, -4.8186, -4.1591, -4.4552, -4.6848, -4.5425, -4.3263, -3.3066, -3.5243, -4.0175, -4.2116, -4.2027, -4.2496, -4.2631, -4.3123, -4.3461, -4.3497, -4.384, -4.4259, -4.5392, -4.5968, -4.6227, -4.6342, -4.7087, -4.7529, -4.822, -4.822, -4.8844, -4.9153, -4.9386, -4.9396, -4.9581, -4.9672, -4.9672, -4.9623, -4.9899, -5.0105, -3.1533, -4.0002, -4.4606, -4.1314, -3.762, -4.6113, -4.6989, -4.6608, -4.904, -2.7766, -2.896, -3.4554, -3.5642, -3.9782, -4.1193, -4.1195, -4.3627, -4.3622, -4.4456, -4.5915, -4.6728, -4.6951, -4.6958, -4.7459, -4.817, -4.8927, -4.9035, -4.9047, -5.0502, -5.1194, -5.1319, -5.1905, -5.2893, -5.3002, -5.3444, -5.37, -5.3737, -5.4022, -5.4282, -2.5667, -3.8964, -4.482, -4.4105, -5.2787, -5.2281, -3.3305, -3.501, -3.6143, -3.772, -3.9059, -3.9953, -4.0407, -4.0594, -4.0832, -4.1451, -4.3121, -4.3776, -4.3829, -4.3917, -4.4074, -4.4502, -4.5808, -4.6189, -4.6192, -4.6192, -4.6192, -4.63, -4.6751, -4.825, -4.8345, -4.95, -5.0342, -5.0451, -5.0796, -5.1074, -4.0101, -3.7211, -3.6525, -3.6585, -4.6949, -4.712, -3.7052, -3.717, -3.776, -3.9514, -3.9576, -4.1589, -4.3316, -4.383, -4.5137, -4.5482, -4.5806, -4.6435, -4.6435, -4.6433, -4.6437, -4.6438, -4.6438, -4.6438, -4.6438, -4.6438, -4.6438, -4.6438, -4.9182, -5.0397, -5.2652, -5.3822, -5.3822, -5.3822, -5.3822, -3.376, -2.0514, -4.578, -4.3698, -4.2292, -4.8987, -4.7749, -4.7459, -3.2422, -3.3657, -3.3967, -3.4713, -3.8068, -3.8407, -3.8626, -3.9066, -4.0599, -4.2274, -4.2769, -4.3585, -4.3858, -4.413, -4.4916, -4.5179, -4.5207, -4.571, -4.6228, -4.6228, -4.6372, -4.6979, -4.6979, -4.7628, -4.7737, -4.9005, -4.9177, -4.9177, -5.0662, -5.12, -3.9205, -4.4784, -3.2404, -3.4671, -3.4699, -3.5383, -3.5719, -3.6713, -3.7914, -3.9085, -3.9289, -4.0566, -4.1972, -4.2289, -4.369, -4.3756, -4.3943, -4.4017, -4.4783, -4.4783, -4.5004, -4.6021, -4.6713, -4.2219, -4.8993, -5.0308, -5.0539, -5.198, -5.317, -5.3245, -5.37, -5.3877, -4.3487, -4.6969], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.4874, 1.4792, 1.479, 1.4744, 1.4736, 1.4735, 1.4729, 1.4723, 1.4714, 1.4695, 1.469, 1.4654, 1.4637, 1.4621, 1.4609, 1.4601, 1.4601, 1.4565, 1.4555, 1.4547, 1.4544, 1.4525, 1.4521, 1.4503, 1.4471, 1.4464, 1.445, 1.4434, 1.4398, 1.4393, 1.3807, 1.3223, 1.218, 0.768, 1.7546, 1.7497, 1.7495, 1.7479, 1.7451, 1.7441, 1.7431, 1.7402, 1.7386, 1.7364, 1.7359, 1.7351, 1.7348, 1.7268, 1.7264, 1.7262, 1.7254, 1.7249, 1.7237, 1.722, 1.7212, 1.7211, 1.7202, 1.7196, 1.7145, 1.7142, 1.7133, 1.7132, 1.7128, 1.7124, 1.6312, 1.6287, 1.6538, 1.4983, 1.0939, 2.0109, 2.0063, 1.9913, 1.9831, 1.9831, 1.9813, 1.9805, 1.9782, 1.9763, 1.9763, 1.9744, 1.9722, 1.9655, 1.9617, 1.96, 1.9591, 1.954, 1.9506, 1.9452, 1.9452, 1.9392, 1.9372, 1.9351, 1.9344, 1.9333, 1.9325, 1.9325, 1.9309, 1.9303, 1.9283, 1.7991, 1.6007, 1.5913, 1.2888, 0.3043, 1.2855, 1.4189, 0.5444, 1.198, 2.2225, 2.2208, 2.2093, 2.2062, 2.1913, 2.1847, 2.1847, 2.1709, 2.1701, 2.1656, 2.1549, 2.1481, 2.1464, 2.146, 2.142, 2.1354, 2.1276, 2.1267, 2.1263, 2.1103, 2.102, 2.1004, 2.0928, 2.079, 2.077, 2.0708, 2.0668, 2.0663, 2.0618, 2.0576, 1.9518, 1.4064, 1.1189, -0.3442, 1.7793, 0.997, 2.261, 2.2562, 2.2529, 2.2475, 2.2419, 2.238, 2.2359, 2.235, 2.2338, 2.2305, 2.2208, 2.2165, 2.2162, 2.2157, 2.2144, 2.2114, 2.2017, 2.1985, 2.1982, 2.1982, 2.1982, 2.1973, 2.1935, 2.1796, 2.1786, 2.1662, 2.1562, 2.155, 2.1506, 2.1467, 1.965, 1.7454, 1.5954, 0.86, 1.4446, 1.4058, 2.3309, 2.3302, 2.3281, 2.3199, 2.3192, 2.3088, 2.2974, 2.2939, 2.2837, 2.2808, 2.278, 2.2723, 2.2723, 2.2722, 2.2722, 2.2721, 2.2721, 2.2721, 2.2721, 2.2721, 2.2721, 2.2721, 2.2431, 2.2278, 2.1948, 2.1749, 2.1749, 2.1749, 2.1749, 2.1719, 2.0148, 1.9405, 1.7322, 1.6677, 1.8982, 1.5437, 0.2065, 2.3475, 2.3442, 2.3432, 2.3408, 2.3287, 2.3273, 2.3264, 2.3243, 2.3166, 2.3064, 2.3035, 2.2975, 2.2955, 2.2935, 2.2866, 2.2854, 2.2839, 2.2808, 2.2757, 2.2757, 2.2748, 2.2691, 2.2691, 2.2622, 2.2615, 2.2473, 2.2452, 2.2452, 2.226, 2.2184, 0.4592, 0.8243, 2.5051, 2.4977, 2.4975, 2.4951, 2.4938, 2.4897, 2.4836, 2.4779, 2.4767, 2.469, 2.4592, 2.4572, 2.4461, 2.4449, 2.4431, 2.4429, 2.4361, 2.4361, 2.4344, 2.424, 2.4167, 2.392, 2.3805, 2.3689, 2.3664, 2.3423, 2.3208, 2.3193, 2.3102, 2.3024, 2.07, -0.6306]}, \"token.table\": {\"Topic\": [2, 3, 5, 4, 8, 1, 4, 4, 7, 6, 8, 4, 3, 7, 1, 6, 5, 5, 8, 5, 3, 7, 8, 3, 7, 7, 3, 5, 8, 8, 8, 1, 4, 8, 7, 8, 3, 3, 1, 4, 5, 1, 1, 4, 2, 1, 6, 8, 2, 3, 4, 7, 2, 7, 4, 8, 5, 3, 4, 4, 3, 5, 3, 2, 6, 7, 3, 2, 8, 1, 3, 7, 2, 1, 6, 2, 4, 3, 6, 2, 4, 5, 7, 5, 5, 5, 7, 4, 5, 3, 1, 3, 3, 6, 2, 5, 5, 6, 2, 3, 6, 8, 5, 2, 8, 7, 2, 2, 5, 8, 4, 1, 1, 8, 7, 8, 3, 1, 5, 1, 8, 4, 3, 4, 6, 8, 1, 5, 7, 3, 1, 6, 4, 6, 5, 5, 3, 6, 2, 8, 2, 3, 5, 4, 5, 3, 8, 6, 1, 8, 1, 8, 1, 5, 7, 3, 4, 5, 1, 7, 3, 1, 3, 4, 6, 5, 4, 1, 2, 4, 8, 7, 1, 3, 4, 3, 5, 6, 7, 2, 4, 6, 2, 6, 2, 3, 1, 4, 6, 7, 3, 7, 2, 2, 6, 8, 2, 5, 2, 1, 5, 6, 1, 1, 3, 6, 1, 2, 6, 6, 6, 2, 6, 2, 2, 6, 8, 6, 2, 2, 2, 4, 5, 7, 2, 2, 1, 2, 4, 1, 2, 6, 3, 6, 7, 8, 5, 1, 8, 5, 8, 8, 4, 8, 1, 1, 6, 6, 3, 4, 4, 3, 4, 1, 7, 7, 7, 3, 5, 2, 3, 8, 5, 1, 4, 7, 1, 6, 7, 2, 8, 4, 6, 3, 1, 7, 7, 7, 4, 3, 2, 4, 6, 5, 7, 2, 5, 8, 6, 6, 1, 2, 1, 5, 8, 5, 2, 6, 7, 6, 5, 3, 4, 7, 5], \"Freq\": [0.3247132376866603, 0.6494264753733207, 0.9672944438273867, 0.9192257942500323, 0.6683738233262739, 0.9545953034515124, 0.8365355755317255, 0.8582370512468569, 0.946305804558879, 0.9241573723952955, 0.9562658123521284, 0.9972145592750615, 0.903799037836892, 0.9715409823268322, 0.9235963923688629, 0.9244169015732622, 0.8979097395416099, 0.2811366845501364, 0.6559855972836517, 0.9292630394484939, 0.9569339065797837, 0.11391448057091205, 0.9113158445672964, 0.8865166591609953, 0.8680189674936325, 0.981745671292031, 0.5549424051058385, 0.4162068038293789, 0.9230580623605277, 0.9751920400685762, 0.860137590202486, 0.9800860319961145, 0.9496111490623454, 0.9523492045389009, 0.7889927537258988, 0.9876806253104982, 0.8430083741213363, 0.872260365384028, 0.936449854376628, 0.8637543243460857, 0.9773302214579971, 0.9743940018990338, 0.9569236907824843, 0.9193332153676226, 0.9305481715199947, 0.9521825110554163, 0.7964713031633337, 0.6618164060847536, 0.23167887611028556, 0.46335775222057113, 0.30890516814704744, 0.9403987545207595, 0.9097314237396844, 0.9398223920741394, 0.7781719649656352, 0.938714122058217, 0.9185872991264424, 0.8863093835726457, 0.8457464319137514, 0.9689762118612143, 0.9659041170795483, 0.9311848435378342, 0.989325127872915, 0.9526730443591428, 0.9244169015732622, 0.8748064499512697, 0.9096696199201544, 0.9205628167954376, 0.9230580623605277, 0.9843925380156129, 0.9363194308576077, 0.9649983830670886, 0.9727572027075564, 0.948679741114141, 0.9588121870591211, 0.9452645218452335, 0.9449673713482459, 0.5005539622007961, 0.5005539622007961, 0.9712340042227237, 0.9587943227855866, 0.9053885685666676, 0.8813309284619844, 0.9774198490401025, 0.9874561896915974, 0.9835507650370574, 0.9882491137150095, 0.8899625109838591, 0.9455636997787689, 0.9652104169252497, 0.960935652486254, 0.9738949410093509, 0.8927970780775554, 0.9244599846495202, 0.7707031669506685, 0.19267579173766713, 0.8418307703993696, 0.9244599846495202, 0.5179351053216737, 0.48340609830022885, 0.9244599846495202, 0.9074387000350581, 0.971895057992875, 0.9500752481194548, 0.9561148837997269, 0.7889927537258988, 0.9910994633992801, 0.24056999412442598, 0.7217099823732779, 0.8868191387236691, 0.8172797816492298, 0.9857525469018722, 0.9913277186547548, 0.951485097500889, 0.9321459903041344, 0.9548872852329717, 0.9295299683096085, 0.5671046714090623, 0.4253285035567967, 0.9531934564144519, 0.02324862088815736, 0.7331432648720562, 0.17832667527751483, 0.07133067011100594, 0.7043903673461837, 0.04458166881937871, 0.39783281337808385, 0.578665910368122, 0.9729719220441259, 0.9192227550685915, 0.27361183182231663, 0.6840295795557916, 0.7996830978532659, 0.9252404035887656, 0.9774198490401025, 0.8439663581581444, 0.4096786900066633, 0.5462382533422177, 0.8716596722412723, 0.13410148803711883, 0.9097827282241394, 0.9021597291658302, 0.9412793174905918, 0.8584669153783043, 0.8491055107696026, 0.909660291142585, 0.9509120765624057, 0.9244599846495202, 0.9761879560234408, 0.9572077393591268, 0.9792804220823194, 0.7498391220223943, 0.9013600428095521, 0.0676020032107164, 0.022534001070238802, 0.8833779933330141, 0.7567851677778245, 0.23824718244857437, 0.9453830088245428, 0.9687257680380227, 0.9439476961472731, 0.9851422876755913, 0.9181182674030737, 0.7615319669519706, 0.9244599846495202, 0.9343531612670476, 0.8643923051523422, 0.996084819270862, 0.9867307290020195, 0.980154499081004, 0.9415092532284028, 0.9729719220441259, 0.9662167223567398, 0.9459409074777111, 0.9089292989874951, 0.8002290994637817, 0.08651125399608452, 0.10813906749510564, 0.948228266751146, 0.9175291797370562, 0.710397815366266, 0.355198907683133, 0.956330298560551, 0.039033889737165345, 0.977631034935243, 0.8677185654202924, 0.9050488898153788, 0.8552810114268272, 0.8777051375995639, 0.9637148488570453, 0.9396739127143415, 0.908563111657261, 0.9245818391046013, 0.5087216940183584, 0.4239347450152987, 0.9421440337547722, 0.9311524133815154, 0.9103900654456356, 0.9272280262622066, 0.9924266812183818, 0.9774198490401025, 0.9580533521218771, 0.9926201067682588, 0.7518890061256512, 0.22278192774093372, 0.9578791234852174, 0.07846124945171186, 0.07846124945171186, 0.8238431192429746, 0.9706189558847076, 0.9695917113005882, 0.9282152367269377, 0.975904174888252, 0.9819248095014612, 0.9641091794111275, 0.9244599846495202, 0.8546987236290446, 0.924497563297491, 0.9862357119356483, 0.9948610852321025, 0.9332427523332143, 0.9104813534509666, 0.8110221361034857, 0.9749114580323118, 0.9728506067179917, 0.9064083326950341, 0.9538409937339362, 0.9304794677913619, 0.8989747921171889, 0.9834661163789566, 0.3106658521918973, 0.6213317043837946, 0.9639112825576434, 0.8777051375995639, 0.950536778775562, 0.9249638555156287, 0.8817617682983518, 0.9651708298017778, 0.9796425789545564, 0.9558163158807984, 0.9151741306482879, 0.8608447848673395, 0.9461532032377185, 0.9741705310218653, 0.9712868878429711, 0.9846771245049233, 0.8475221214289694, 0.7877724500039311, 0.9330002143141377, 0.9942672132862743, 0.7806980172745029, 0.6619286197282349, 0.33096430986411746, 0.9881206788200649, 0.9884577453877528, 0.9707770595982318, 0.8592903383420474, 0.9321278887301492, 0.8643735127521596, 0.9436789001186346, 0.9285554965807794, 0.9803778011315867, 0.9756063306316366, 0.33771785424793793, 0.4298227235882846, 0.2149113617941423, 0.965302261397059, 0.8760013892149926, 0.8671508954666564, 0.8650115679121695, 0.08650115679121695, 0.9807772469423154, 0.8730497717257222, 0.9295337360222595, 0.8417464808006272, 0.14639069231315255, 0.9209180613225091, 0.9682664247135438, 0.9701386355465195, 0.9705400309342295, 0.849893044744622, 0.09998741702877906, 0.9244599846495202, 0.8461792310638979, 0.908563111657261, 0.970813137539929, 0.8934706627253154, 0.7654454043750456, 0.8777051375995639, 0.9705695926881034, 0.9326961022577119, 0.9478432485485484, 0.49409650507443476, 0.49409650507443476, 0.8630694989945322, 0.818899806242149, 0.9538068641877105, 0.8777051375995639, 0.8978846967974693, 0.9842470958876065, 0.9430214917059989, 0.9700766676659311, 0.7871571320796793, 0.95076168341809, 0.9438733535739431], \"Term\": [\"2\", \"2\", \"able\", \"abuse\", \"addition\", \"address\", \"agree\", \"ahhhh\", \"also\", \"always\", \"amazing\", \"animal\", \"app\", \"appreciate\", \"article\", \"asset\", \"assist\", \"available\", \"available\", \"back\", \"bad\", \"bag\", \"bag\", \"bank\", \"beaultiful\", \"beautiful\", \"best\", \"best\", \"bigger\", \"body\", \"bonus\", \"boot\", \"boycott\", \"brand\", \"bug\", \"buy\", \"came\", \"card\", \"case\", \"cashmere\", \"caught\", \"chat\", \"check\", \"clothing\", \"collection\", \"color\", \"colour\", \"comment\", \"company\", \"company\", \"company\", \"complete\", \"compliment\", \"confirmation\", \"cool\", \"could\", \"country\", \"credit\", \"cruel\", \"cruelty\", \"customer\", \"day\", \"deliver\", \"delivery\", \"digital\", \"direct\", \"disappointed\", \"discount\", \"diversity\", \"dm\", \"dont\", \"doubt\", \"either\", \"email\", \"employee\", \"even\", \"ever\", \"exchange\", \"exchange\", \"experience\", \"exploiting\", \"eye\", \"fan\", \"far\", \"fashion\", \"fashion_friend\", \"fashionista\", \"feather\", \"feedback\", \"feel\", \"find\", \"first\", \"fit\", \"forum\", \"free\", \"free\", \"friend\", \"gdansk\", \"get\", \"get\", \"girlfriend\", \"give\", \"glad\", \"go\", \"good\", \"google\", \"got\", \"great\", \"great\", \"h\", \"hair\", \"happy\", \"hear\", \"heat\", \"hello\", \"help\", \"helpful\", \"hey\", \"hey\", \"hi\", \"hi\", \"high\", \"hm\", \"hm\", \"hm\", \"hm\", \"hope\", \"hope\", \"human\", \"im\", \"india\", \"india\", \"industry\", \"information\", \"inspo\", \"issue\", \"it\", \"it\", \"item\", \"item\", \"ive\", \"jacket\", \"kindly\", \"kneehigh\", \"know\", \"late\", \"later\", \"laughed\", \"let\", \"life\", \"like\", \"long\", \"look\", \"look\", \"look\", \"lost\", \"love\", \"love\", \"made\", \"make\", \"message\", \"model\", \"month\", \"more\", \"morning\", \"much\", \"murdered\", \"need\", \"never\", \"new\", \"nice\", \"non\", \"number\", \"oh\", \"omg\", \"one\", \"one\", \"one\", \"online\", \"option\", \"orange\", \"orange\", \"order\", \"order\", \"ordered\", \"original\", \"out\", \"outfit\", \"overshirt\", \"pain\", \"parcel\", \"part\", \"pay\", \"payment\", \"payment\", \"perfect\", \"peta\", \"pink\", \"place\", \"please\", \"positive\", \"post\", \"price\", \"product\", \"product\", \"proof\", \"purchase\", \"purchase\", \"purchase\", \"put\", \"quality\", \"reach\", \"receipt\", \"received\", \"refund\", \"refused\", \"regard\", \"remove\", \"reply\", \"return\", \"returned\", \"reza\", \"right\", \"said\", \"say\", \"second\", \"see\", \"sell\", \"selling\", \"send\", \"sent\", \"sent\", \"service\", \"shame\", \"share\", \"shoe\", \"shop\", \"shopping\", \"show\", \"similar\", \"site\", \"size\", \"skin\", \"smaller\", \"soon\", \"sorry\", \"statement\", \"sticker\", \"still\", \"stop\", \"stopanimalabuse\", \"store\", \"store\", \"style\", \"sure\", \"sweatshirt\", \"tag\", \"take\", \"talk\", \"tell\", \"telling\", \"term\", \"thank\", \"thanks\", \"thanks\", \"thanks\", \"there\", \"think\", \"this\", \"time\", \"time\", \"trendsetter\", \"turn\", \"twice\", \"u\", \"u\", \"understand\", \"use\", \"using\", \"valuable\", \"vegan\", \"vegan\", \"vinted\", \"wait\", \"walkin\", \"want\", \"wanted\", \"wardrobe\", \"washed\", \"way\", \"website\", \"week\", \"were\", \"were\", \"winter\", \"wish\", \"work\", \"worn\", \"worst\", \"would\", \"wow\", \"write\", \"yes\", \"you\", \"yours\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [8, 6, 1, 5, 7, 2, 3, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el57941397610459420482788394313\", ldavis_el57941397610459420482788394313_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el57941397610459420482788394313\", ldavis_el57941397610459420482788394313_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el57941397610459420482788394313\", ldavis_el57941397610459420482788394313_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "7     -0.379794  0.061971       1        1  22.471196\n",
       "5      0.202921  0.273433       2        1  16.946675\n",
       "0      0.357545 -0.057384       3        1  13.130845\n",
       "4      0.126201 -0.026070       4        1  10.685817\n",
       "6     -0.191436 -0.246083       5        1  10.169309\n",
       "1     -0.074058  0.292598       6        1   9.342910\n",
       "2      0.080435 -0.316096       7        1   9.324148\n",
       "3     -0.121814  0.017632       8        1   7.929100, topic_info=           Term        Freq       Total Category  logprob  loglift\n",
       "37           hm  112.000000  112.000000  Default  30.0000  30.0000\n",
       "22       please  103.000000  103.000000  Default  29.0000  29.0000\n",
       "1          love   71.000000   71.000000  Default  28.0000  28.0000\n",
       "1221     animal   44.000000   44.000000  Default  27.0000  27.0000\n",
       "127           u   81.000000   81.000000  Default  26.0000  26.0000\n",
       "...         ...         ...         ...      ...      ...      ...\n",
       "475        life    2.526986    3.134116   Topic8  -5.3245   2.3193\n",
       "859     comment    2.414537    3.021986   Topic8  -5.3700   2.3102\n",
       "1243   addition    2.372119    2.992337   Topic8  -5.3877   2.3024\n",
       "283   available    6.705092   10.670966   Topic8  -4.3487   2.0700\n",
       "37           hm    4.733408  112.153720   Topic8  -4.6969  -0.6306\n",
       "\n",
       "[311 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "826       2  0.324713         2\n",
       "826       3  0.649426         2\n",
       "104       5  0.967294      able\n",
       "1373      4  0.919226     abuse\n",
       "1243      8  0.668374  addition\n",
       "...     ...       ...       ...\n",
       "369       5  0.943021       wow\n",
       "1557      3  0.970077     write\n",
       "67        4  0.787157       yes\n",
       "23        7  0.950762       you\n",
       "670       5  0.943873     yours\n",
       "\n",
       "[305 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[8, 6, 1, 5, 7, 2, 3, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus2, id2word, mds=\"mmds\", R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
